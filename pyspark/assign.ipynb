{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89708bf1",
   "metadata": {},
   "source": [
    "Pyspark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938caa10",
   "metadata": {},
   "source": [
    "Working with DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a65dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, udf, avg, count, max, broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc81a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a PySpark DataFrame manually using a list of tuples and a defined schema\n",
    "data = [\n",
    "    (1, \"Aliya\", 23),\n",
    "    (2, \"Binda\", 29),\n",
    "    (3, \"Rita\", 26),\n",
    "    (4, \"David\", 32),\n",
    "    (5, \"Sita\", 34)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a658c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+---+-----+---+\n",
      "| ID| Name|Age|\n",
      "+---+-----+---+\n",
      "|  1|Aliya| 23|\n",
      "|  2|Binda| 29|\n",
      "|  3| Rita| 26|\n",
      "|  4|David| 32|\n",
      "|  5| Sita| 34|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Display the schema and the first 5 rows of the DataFrame. Use .printSchema() and .show() to inspect the structure and data visually\n",
    "df.printSchema() \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a76322",
   "metadata": {},
   "source": [
    "Transformations and Actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d53a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Aliya| 23|\n",
      "|Binda| 29|\n",
      "| Rita| 26|\n",
      "|David| 32|\n",
      "| Sita| 34|\n",
      "+-----+---+\n",
      "\n",
      "+---+-----+---+\n",
      "| ID| Name|Age|\n",
      "+---+-----+---+\n",
      "|  2|Binda| 29|\n",
      "|  3| Rita| 26|\n",
      "|  4|David| 32|\n",
      "|  5| Sita| 34|\n",
      "+---+-----+---+\n",
      "\n",
      "+---+-----+---+----------+\n",
      "| ID| Name|Age|Age_plus_5|\n",
      "+---+-----+---+----------+\n",
      "|  1|Aliya| 23|        28|\n",
      "|  2|Binda| 29|        34|\n",
      "|  3| Rita| 26|        31|\n",
      "|  4|David| 32|        37|\n",
      "|  5| Sita| 34|        39|\n",
      "+---+-----+---+----------+\n",
      "\n",
      "+---+-----+\n",
      "| ID| Name|\n",
      "+---+-----+\n",
      "|  1|Aliya|\n",
      "|  2|Binda|\n",
      "|  3| Rita|\n",
      "|  4|David|\n",
      "|  5| Sita|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Perform transformations such as .select(), .filter(), .withColumn(), and .drop() on a sample DataFrame.\n",
    "df.select(\"Name\", \"Age\").show()\n",
    "df.filter(col(\"Age\") > 25).show()\n",
    "df.withColumn(\"Age_plus_5\", col(\"Age\") + 5).show()\n",
    "df.drop(\"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86236222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5\n",
      "First row: Row(ID=1, Name='Aliya', Age=23)\n",
      "Take 3 rows: [Row(ID=1, Name='Aliya', Age=23), Row(ID=2, Name='Binda', Age=29), Row(ID=3, Name='Rita', Age=26)]\n"
     ]
    }
   ],
   "source": [
    "# 2. Use .collect(), .count(), .first() and .take(n) as actions.\n",
    "print(\"Total rows:\", df.count())\n",
    "print(\"First row:\", df.first())\n",
    "print(\"Take 3 rows:\", df.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f768ed",
   "metadata": {},
   "source": [
    "Schema Inference and Manual Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf079ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- major: string (nullable = true)\n",
      " |-- GPA: double (nullable = true)\n",
      " |-- course_load: integer (nullable = true)\n",
      " |-- avg_course_grade: double (nullable = true)\n",
      " |-- attendance_rate: double (nullable = true)\n",
      " |-- enrollment_status: string (nullable = true)\n",
      " |-- lms_logins_past_month: integer (nullable = true)\n",
      " |-- avg_session_duration_minutes: integer (nullable = true)\n",
      " |-- assignment_submission_rate: double (nullable = true)\n",
      " |-- forum_participation_count: integer (nullable = true)\n",
      " |-- video_completion_rate: double (nullable = true)\n",
      " |-- risk_level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a CSV file with schema inference enabled. Print the inferred schema.\n",
    "csv_path = \"E:\\Desktop\\GRITFEAT\\pandas\\student.csv\" \n",
    "\n",
    "# With inference\n",
    "df_csv_infer = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df_csv_infer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8431064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the same CSV with a manually defined schema. Compare both results\n",
    "manual_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "df_csv_manual = spark.read.csv(csv_path, header=True, schema=manual_schema)\n",
    "df_csv_manual.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4a217",
   "metadata": {},
   "source": [
    "Reading/Writing Data (CSV, JSON, Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eea0dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read a CSV file into a DataFrame using spark.read.csv(). Include header and inferSchema options.\n",
    "\n",
    "df_csv = spark.read.csv(csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d17f1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read a JSON file using spark.read.json().\n",
    "df_json = spark.read.json(\"C:\\\\sample1.json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d9064",
   "metadata": {},
   "source": [
    "Filtering, Joins, Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eae303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| ID| Name|Age|\n",
      "+---+-----+---+\n",
      "|  2|Binda| 29|\n",
      "|  4|David| 32|\n",
      "|  5| Sita| 34|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Filter rows where a column value is greater than a given threshold.\n",
    "df.filter(col(\"Age\") > 26).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175e21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+\n",
      "| ID| Name|Age|   Dept|\n",
      "+---+-----+---+-------+\n",
      "|  1|Aliya| 23|     HR|\n",
      "|  2|Binda| 29|     IT|\n",
      "|  3| Rita| 26|Finance|\n",
      "|  4|David| 32|     IT|\n",
      "|  5| Sita| 34|     HR|\n",
      "+---+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Perform an inner join between two DataFrames on a common column\n",
    "df2 = spark.createDataFrame([\n",
    "    (1, \"HR\"),\n",
    "    (2, \"IT\"),\n",
    "    (3, \"Finance\"),\n",
    "    (4, \"IT\"),\n",
    "    (5, \"HR\")\n",
    "], [\"ID\", \"Dept\"])\n",
    "\n",
    "df.join(df2, \"ID\", \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "722ee1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+\n",
      "| ID| Name|Age|   Dept|\n",
      "+---+-----+---+-------+\n",
      "|  1|Aliya| 23|     HR|\n",
      "|  2|Binda| 29|     IT|\n",
      "|  3| Rita| 26|Finance|\n",
      "|  4|David| 32|     IT|\n",
      "|  5| Sita| 34|     HR|\n",
      "+---+-----+---+-------+\n",
      "\n",
      "+---+-----+---+-------+\n",
      "| ID| Name|Age|   Dept|\n",
      "+---+-----+---+-------+\n",
      "|  1|Aliya| 23|     HR|\n",
      "|  2|Binda| 29|     IT|\n",
      "|  3| Rita| 26|Finance|\n",
      "|  4|David| 32|     IT|\n",
      "|  5| Sita| 34|     HR|\n",
      "+---+-----+---+-------+\n",
      "\n",
      "+---+-----+---+-------+\n",
      "| ID| Name|Age|   Dept|\n",
      "+---+-----+---+-------+\n",
      "|  1|Aliya| 23|     HR|\n",
      "|  2|Binda| 29|     IT|\n",
      "|  3| Rita| 26|Finance|\n",
      "|  4|David| 32|     IT|\n",
      "|  5| Sita| 34|     HR|\n",
      "+---+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Perform left outer, right outer, and full outer joins and observe differences.\n",
    "df.join(df2, \"ID\", \"left\").show()\n",
    "df.join(df2, \"ID\", \"right\").show()\n",
    "df.join(df2, \"ID\", \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a33f79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------+\n",
      "|Age|avg_age|count_id|max_age|\n",
      "+---+-------+--------+-------+\n",
      "| 23|   23.0|       1|     23|\n",
      "| 29|   29.0|       1|     29|\n",
      "| 26|   26.0|       1|     26|\n",
      "| 32|   32.0|       1|     32|\n",
      "| 34|   34.0|       1|     34|\n",
      "+---+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Group data by a category column and compute average, count, and max of numeric columns.\n",
    "df.groupBy(\"Age\").agg(\n",
    "    avg(\"Age\").alias(\"avg_age\"),\n",
    "    count(\"ID\").alias(\"count_id\"),\n",
    "    max(\"Age\").alias(\"max_age\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|Age|count(ID)|\n",
      "+---+---------+\n",
      "| 29|        1|\n",
      "| 26|        1|\n",
      "| 32|        1|\n",
      "| 34|        1|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Chain transformations like .filter() → .groupBy() → .agg() in a single statement.\n",
    "df.filter(col(\"Age\") > 24).groupBy(\"Age\").agg(count(\"ID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146b8d6",
   "metadata": {},
   "source": [
    "User-Defined Functions (UDFs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca343e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a simple Python function to categorize numerical columns into groups and convert it into a PySpark UDF.\n",
    "def age_group(age):\n",
    "    if age < 25:\n",
    "        return \"Young\"\n",
    "    elif age < 30:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"Senior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ff89384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------+\n",
      "| ID| Name|Age|AgeGroup|\n",
      "+---+-----+---+--------+\n",
      "|  1|Aliya| 23|   Young|\n",
      "|  2|Binda| 29|     Mid|\n",
      "|  3| Rita| 26|     Mid|\n",
      "|  4|David| 32|  Senior|\n",
      "|  5| Sita| 34|  Senior|\n",
      "+---+-----+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Use your UDF in a .withColumn() transformation to apply it on a DataFrame column.\n",
    "age_group_udf = udf(age_group, StringType())\n",
    "df.withColumn(\"AgeGroup\", age_group_udf(col(\"Age\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc7950",
   "metadata": {},
   "source": [
    "Spark SQL (Querying DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc34cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Register a DataFrame as a temporary view using .createOrReplaceTempView().\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999615e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Use spark.sql() to run SQL queries on the registered view\n",
    "spark.sql(\"SELECT COUNT(*) FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|Age|cnt|\n",
      "+---+---+\n",
      "| 23|  1|\n",
      "| 29|  1|\n",
      "| 26|  1|\n",
      "| 32|  1|\n",
      "| 34|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Write an SQL query to count rows, group by category, and sort the results.\n",
    "spark.sql(\"SELECT Age, COUNT(*) as cnt FROM people GROUP BY Age ORDER BY cnt DESC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What are the advantages of using Spark SQL over native DataFrame operations?\n",
    "\n",
    "# - Easier for people with SQL background\n",
    "# - Integrates with BI tools\n",
    "# - Can optimize queries internally (Catalyst optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c53eb",
   "metadata": {},
   "source": [
    "Optimization and Performance Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c1d56",
   "metadata": {},
   "source": [
    "Partitioning and Bucketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef24de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use .repartition() and .coalesce() to change the number of partitions in a DataFrame. Observe the effect.\n",
    "df_repart = df.repartition(4)\n",
    "df_coalesce = df_repart.coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd9840",
   "metadata": {},
   "source": [
    "Caching and Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01ac3c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| ID| Name|Age|\n",
      "+---+-----+---+\n",
      "|  1|Aliya| 23|\n",
      "|  2|Binda| 29|\n",
      "|  3| Rita| 26|\n",
      "|  4|David| 32|\n",
      "|  5| Sita| 34|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Cache a DataFrame using .cache().\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f949693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| ID| Name|Age|\n",
      "+---+-----+---+\n",
      "|  1|Aliya| 23|\n",
      "|  2|Binda| 29|\n",
      "|  3| Rita| 26|\n",
      "|  4|David| 32|\n",
      "|  5| Sita| 34|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Use .persist().\n",
    "df.persist()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "287a5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+\n",
      "| ID| Name|Age|   Dept|\n",
      "+---+-----+---+-------+\n",
      "|  1|Aliya| 23|     HR|\n",
      "|  2|Binda| 29|     IT|\n",
      "|  3| Rita| 26|Finance|\n",
      "|  4|David| 32|     IT|\n",
      "|  5| Sita| 34|     HR|\n",
      "+---+-----+---+-------+\n",
      "\n",
      "+---+-----+---+-------+\n",
      "| ID| Name|Age|   Dept|\n",
      "+---+-----+---+-------+\n",
      "|  1|Aliya| 23|     HR|\n",
      "|  2|Binda| 29|     IT|\n",
      "|  3| Rita| 26|Finance|\n",
      "|  4|David| 32|     IT|\n",
      "|  5| Sita| 34|     HR|\n",
      "+---+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Perform a broadcast join and observe performance improvements when joining a large and small DataFrame.\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_large = df.join(df2, \"ID\")\n",
    "df_broadcast = df.join(broadcast(df2), \"ID\")\n",
    "df_large.show()\n",
    "df_broadcast.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gritfeat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
